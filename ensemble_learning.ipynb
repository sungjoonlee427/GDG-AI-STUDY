{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 학습(Ensemble Learning)\n",
    "\n",
    "앙상블 학습은 여러 개의 모델을 결합하여 더 나은 성능을 얻는 기법임. 개별 모델의 예측을 결합해 오차를 줄이고 일반화 성능을 향상시킴.\n",
    "\n",
    "주요 방식:\n",
    "- **배깅(Bagging)**: 여러 모델을 병렬로 학습시키고 예측을 평균 또는 투표로 결합함. 대표적으로 **랜덤 포레스트(Random Forest)** 가 있음.\n",
    "- **부스팅(Boosting)**: 모델을 순차적으로 학습시키며 이전 모델이 잘못 예측한 데이터에 가중치를 부여함. 대표적으로 **AdaBoost**, **Gradient Boosting**이 있음.\n",
    "- **스태킹(Stacking)**: 서로 다른 모델들의 예측 결과를 메타 모델이 결합하여 최종 예측을 함.\n",
    "\n",
    "앙상블 기법은 정형 데이터(structured data)개별 모델의 한계를 보완하여 성능을 높이는 데 유용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트(Random Forest)\n",
    "\n",
    "랜덤 포레스트는 **배깅(Bagging)** 기반의 앙상블 학습 기법으로, 여러 개의 **결정 트리(Decision Tree)** 를 결합하여 예측 성능을 높임. \n",
    "\n",
    "작동 방식:\n",
    "1. **부트스트랩 샘플링**: 원본 데이터에서 여러 샘플을 **부트스트랩 샘플** 방식으로 무작위 추출함. 이는 데이터 셋에서 중복을 허용하여 일부 데이터를 여러 번 사용하거나 아예 사용하지 않을 수 있게 하는 방법으로, 각 트리마다 다른 학습 데이터 세트를 구성하여 모델의 다양성을 높임.\n",
    "2. **특성 무작위 선택**: 각 트리의 분기 시, 일부 랜덤하게 선택된 특성들 중에서 분할을 수행하여 트리 간의 상관성을 낮추고 다양성을 높임.\n",
    "3. **예측 결합**: 분류 문제에서는 다수결 투표로, 회귀 문제에서는 평균을 내어 최종 예측을 만듦.\n",
    "\n",
    "부트스트랩 샘플링을 통해 데이터 변동에 강하고, 과적합을 줄이며 높은 예측 성능을 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 배열 및 수치 연산을 위한 라이브러리\n",
    "import pandas as pd  # 데이터 분석을 위한 라이브러리\n",
    "from sklearn.model_selection import train_test_split  # 데이터셋을 학습용과 테스트용으로 나누는 함수\n",
    "\n",
    "# 와인 데이터 불러옴. 데이터는 CSV 파일로 제공되며 URL을 통해 직접 다운로드\n",
    "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
    "\n",
    "# 독립 변수 선택: 'alcohol', 'sugar', 'pH' 컬럼만 사용해 모델을 훈련\n",
    "# DataFrame에서 원하는 컬럼을 선택한 후, 이를 넘파이 배열로 변환\n",
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
    "\n",
    "# 종속 변수 선택: 'class' 컬럼을 사용해 와인의 클래스를 예측\n",
    "# DataFrame의 'class' 컬럼을 넘파이 배열로 변환\n",
    "target = wine['class'].to_numpy()\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 나눔\n",
    "# test_size=0.2는 전체 데이터의 20%를 테스트 세트로, 나머지 80%를 학습 세트로 사용\n",
    "# random_state=42는 결과의 재현성을 위해 난수 시드를 고정함\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8903229806766861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate  # 모델의 교차 검증을 위한 함수\n",
    "from sklearn.ensemble import RandomForestClassifier  # 랜덤 포레스트 분류 모델\n",
    "\n",
    "# 랜덤 포레스트 분류 모델 생성\n",
    "# n_jobs=-1로 설정해 모든 CPU 코어를 사용해 병렬 처리\n",
    "# random_state=42로 설정해 결과의 재현성을 보장\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계에서의 점수도 반환\n",
    "# n_jobs=-1로 설정해 교차 검증을 병렬 처리\n",
    "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23183515 0.50059756 0.26756729]\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트 모델을 학습 데이터로 훈련\n",
    "rf.fit(train_input, train_target)\n",
    "\n",
    "# 특성 중요도 출력\n",
    "# 모델이 각 특성('alcohol', 'sugar', 'pH')에 대해 예측에 얼마나 기여했는지를 나타내는 중요도 값을 반환\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8945545507023283\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트 분류 모델 생성\n",
    "# oob_score=True로 설정해 OOB (Out-of-Bag) 샘플을 사용한 평가 점수를 계산\n",
    "# n_jobs=-1로 모든 CPU 코어를 사용해 병렬 처리, random_state=42로 결과의 재현성 보장\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# 학습 데이터로 모델 훈련\n",
    "rf.fit(train_input, train_target)\n",
    "\n",
    "# OOB 샘플을 사용해 계산한 점수 출력\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엑스트라 트리(Extra Trees, Extremely Randomized Trees)\n",
    "\n",
    "엑스트라 트리는 **랜덤 포레스트** 와 유사한 앙상블 학습 기법으로, 다수의 **결정 트리(Decision Tree)** 를 결합하여 예측 성능을 높임. 그러나 랜덤 포레스트와 달리, 분할 시 무작위성을 더욱 강화하여 모델의 변동성을 낮춤.\n",
    "\n",
    "작동 방식:\n",
    "1. **전체 데이터 사용**: 랜덤 포레스트와 달리, 부트스트랩 샘플링을 사용하지 않고 원본 데이터 전체를 사용하여 각 트리를 학습시킴.\n",
    "2. **특성 무작위 선택과 무작위 분할**: 각 트리의 분기 시 일부 랜덤하게 선택된 특성 중, 최적의 분할 값 대신 임의의 분할 값을 사용하여 트리의 분할을 수행함. 이로써 모델의 무작위성을 높이고, 트리 간의 상관성을 낮춤.\n",
    "3. **예측 결합**: 랜덤 포레스트와 마찬가지로, 분류 문제에서는 다수결 투표, 회귀 문제에서는 평균을 내어 최종 예측을 만듦.\n",
    "\n",
    "엑스트라 트리는 계산 속도가 빠르고, 데이터 변동성에 강하며, 과적합을 줄이는 효과가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier  # 엑스트라 트리 분류 모델\n",
    "\n",
    "# 엑스트라 트리 분류 모델 생성\n",
    "# n_jobs=-1로 모든 CPU 코어를 사용해 병렬 처리\n",
    "# random_state=42로 결과의 재현성 보장\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계의 점수도 반환\n",
    "# n_jobs=-1로 설정해 교차 검증을 병렬 처리\n",
    "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "# 엑스트라 트리 모델을 학습 데이터로 훈련\n",
    "et.fit(train_input, train_target)\n",
    "\n",
    "# 특성 중요도 출력\n",
    "# 모델이 각 특성('alcohol', 'sugar', 'pH')에 대해 예측에 얼마나 기여했는지를 나타내는 중요도 값을 반환\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그레이디언트 부스팅(Gradient Boosting)\n",
    "\n",
    "그레이디언트 부스팅은 **부스팅(Boosting)** 기반의 앙상블 학습 기법으로, 여러 약한 학습기(주로 결정 트리)를 순차적으로 학습시켜 강력한 예측 모델을 만드는 방식임.\n",
    "\n",
    "작동 방식:\n",
    "1. **순차적 학습**: 모델을 순차적으로 학습하며, 이전 모델의 오차(잔차)를 다음 모델이 보완하도록 함.\n",
    "2. **잔차 최소화**: 각 단계에서 이전 모델의 예측 오차를 줄이기 위해, 손실 함수의 **그레이디언트(Gradient)** 를 계산하여 이를 보정할 수 있는 새로운 모델을 학습시킴.\n",
    "3. **모델 결합**: 최종 예측은 모든 모델의 예측을 가중합으로 결합하여 계산함.\n",
    "\n",
    "특징:\n",
    "- **고성능**: 이전 단계의 오차를 보정하며 학습하기 때문에 복잡한 데이터에 대해 높은 예측 성능을 가짐.\n",
    "- **하이퍼파라미터 중요성**: 학습률(Learning Rate)이나 트리의 깊이 등의 하이퍼파라미터가 모델 성능에 큰 영향을 미치므로, 적절한 튜닝이 필요함.\n",
    "- 대표적으로 **XGBoost**, **LightGBM**, **CatBoost**와 같은 변형 모델이 있음.\n",
    "\n",
    "그레이디언트 부스팅은 다양한 문제에 강력한 성능을 제공하지만, 계산 비용이 크므로 주의가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  # 그레이디언트 부스팅 분류 모델\n",
    "\n",
    "# 그레이디언트 부스팅 분류 모델 생성\n",
    "# random_state=42로 결과의 재현성 보장\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계에서의 점수도 반환\n",
    "# n_jobs=-1로 설정해 교차 검증을 병렬 처리\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464595437171814 0.8780082549788999\n"
     ]
    }
   ],
   "source": [
    "# 그레이디언트 부스팅 분류 모델 생성\n",
    "# n_estimators=500: 사용할 트리의 개수를 500개로 설정하여 더 많은 학습을 수행\n",
    "# learning_rate=0.2: 학습률을 0.2로 설정하여 각 단계의 기여를 조정\n",
    "# random_state=42: 결과의 재현성을 보장\n",
    "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계의 점수도 반환\n",
    "# n_jobs=-1로 설정해 교차 검증을 병렬 처리\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15881044 0.67988912 0.16130044]\n"
     ]
    }
   ],
   "source": [
    "# 그레이디언트 부스팅 모델을 학습 데이터로 훈련\n",
    "gb.fit(train_input, train_target)\n",
    "\n",
    "# 특성 중요도 출력\n",
    "# 모델이 각 특성('alcohol', 'sugar', 'pH')에 대해 예측에 얼마나 기여했는지를 나타내는 중요도 값을 반환\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)\n",
    "\n",
    "히스토그램 기반 그레이디언트 부스팅은 **그레이디언트 부스팅** 의 한 변형으로, 학습 속도를 높이기 위해 **히스토그램 분할 방식** 을 사용하여 연속형 특성을 구간으로 나누어 처리함. 대용량 데이터에 특히 효율적임.\n",
    "\n",
    "작동 방식:\n",
    "1. **히스토그램 분할**: 연속형 특성을 일정 구간으로 나누어 **히스토그램(bin)** 을 생성함. 이를 통해 데이터의 분포를 빠르게 파악하고, 분할 기준을 효율적으로 선택할 수 있음.\n",
    "2. **그레이디언트 기반 분할**: 각 히스토그램 구간 내에서 그레이디언트를 계산하여, 최적의 분할점을 선택하고 오차를 줄임.\n",
    "3. **모델 결합**: 일반적인 그레이디언트 부스팅과 마찬가지로, 모든 모델의 예측을 가중합하여 최종 예측을 만듦.\n",
    "\n",
    "특징:\n",
    "- **속도 향상**: 히스토그램을 사용하여 계산량을 줄이므로, 대용량 데이터에서 학습 속도가 빠름.\n",
    "- **메모리 절약**: 데이터 전체가 아닌 구간별 빈(bin)만을 저장하므로, 메모리 사용량이 줄어듦.\n",
    "- **고성능**: XGBoost, LightGBM, CatBoost 등 최신 그레이디언트 부스팅 모델에서 활용되며, 특히 LightGBM은 이 방식을 최적화하여 효율성과 성능을 모두 극대화함.\n",
    "\n",
    "히스토그램 기반 그레이디언트 부스팅은 대규모 데이터셋에서의 효율성과 정확성을 동시에 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leesung-joon/jupyter_env/lib/python3.13/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321723946453317 0.8801241948619236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # 히스토그램 기반 그레이디언트 부스팅을 활성화\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier  # 히스토그램 기반 그레이디언트 부스팅 분류 모델\n",
    "\n",
    "# 히스토그램 기반 그레이디언트 부스팅 모델 생성\n",
    "# random_state=42로 설정해 결과의 재현성을 보장\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계에서의 점수도 반환\n",
    "scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08876275 0.23438522 0.08027708]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance  # 특성 중요도를 평가하기 위한 함수\n",
    "\n",
    "# 히스토그램 기반 그레이디언트 부스팅 모델을 학습 데이터로 훈련\n",
    "hgb.fit(train_input, train_target)\n",
    "\n",
    "# 순열 중요도 계산\n",
    "# n_repeats=10: 각 특성에 대해 10번의 반복을 통해 중요도를 평가\n",
    "# random_state=42: 결과의 재현성을 보장\n",
    "# n_jobs=-1: 가능한 모든 CPU 코어를 사용하여 병렬 처리\n",
    "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 각 특성의 중요도 평균값 출력\n",
    "# importances_mean은 각 특성에 대한 중요도의 평균값을 반환\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05969231 0.20238462 0.049     ]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋을 사용하여 순열 중요도 계산\n",
    "# test_input과 test_target을 사용해 모델 성능에 대한 특성 중요도를 평가\n",
    "# n_repeats=10: 각 특성에 대해 10번의 반복을 통해 중요도를 평가\n",
    "# random_state=42: 결과의 재현성을 보장\n",
    "# n_jobs=-1: 가능한 모든 CPU 코어를 사용하여 병렬 처리\n",
    "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 각 특성의 중요도 평균값 출력\n",
    "# importances_mean은 각 특성에 대한 중요도의 평균값을 반환\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723076923076923"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터셋을 사용하여 모델의 성능 평가\n",
    "# hgb 모델에 대해 test_input(특성)과 test_target(타겟) 데이터를 사용하여 정확도(score)를 계산\n",
    "score = hgb.score(test_input, test_target)\n",
    "\n",
    "# 계산된 정확도 출력\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost는 **그레이디언트 부스팅(Gradient Boosting)** 을 기반으로 한 고성능 머신러닝 라이브러리로, 속도와 효율성을 극대화한 다양한 최적화 기법이 포함되어 있음. 주로 대용량 데이터와 복잡한 예측 문제에서 높은 성능을 발휘함.\n",
    "\n",
    "작동 방식:\n",
    "1. **그레이디언트 부스팅 기반 학습**: 기본적으로 그레이디언트 부스팅의 잔차를 최소화하는 방식으로 학습하며, 각 단계에서 이전 모델의 오차를 보정하는 새로운 모델을 추가함.\n",
    "2. **정규화 및 가지치기**: 모델이 과적합되지 않도록 L1, L2 정규화와 함께, 불필요한 분기를 제거하는 **가지치기(pruning)**  기법을 사용함.\n",
    "3. **병렬 처리**: 데이터의 여러 특성에 대해 병렬 처리가 가능하도록 하여 학습 속도를 크게 향상시킴.\n",
    "4. **히스토그램 기반 분할**: 연속형 특성을 구간으로 나눠 히스토그램을 생성해 분할점을 선택하므로, 메모리와 계산 비용을 절약함.\n",
    "\n",
    "특징:\n",
    "- **고성능**: 정규화와 가지치기로 모델의 일반화 성능을 높이고, 빠른 학습과 예측을 지원함.\n",
    "- **유연한 파라미터 조정**: 학습률(Learning Rate), 트리 깊이, 정규화 계수 등 다양한 하이퍼파라미터를 통해 모델을 세밀하게 튜닝할 수 있음.\n",
    "- **강력한 확장성**: 대용량 데이터와 고차원 특성을 가진 데이터에 대해서도 뛰어난 성능을 보임.\n",
    "\n",
    "XGBoost는 데이터 과학 경진 대회에서 자주 사용되는 모델로, 높은 예측 성능과 효율성 덕분에 널리 활용되고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558403027491312 0.8782000074035686\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier  # XGBoost 분류 모델 임포트\n",
    "\n",
    "# XGBoost 모델 생성: 'tree_method'를 'hist'로 설정하여 히스토그램 기반 방식 사용\n",
    "# random_state=42로 설정해 결과의 재현성을 보장\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계에서의 점수도 반환\n",
    "scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
    "\n",
    "# 학습 점수와 테스트 점수의 평균 출력\n",
    "# train_score: 훈련 데이터에 대한 성능, test_score: 검증 데이터에 대한 성능\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006\n",
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] Number of positive: 3152, number of negative: 1006\n",
      "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
      "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
      "[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738\n",
      "[LightGBM] [Info] Start training from score 1.140744\n",
      "\n",
      "[LightGBM] [Info] Start training from score 1.141738\n",
      "[LightGBM] [Info] Start training from score 1.141738\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 371\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.758057 -> initscore=1.142055\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744\n",
      "[LightGBM] [Info] Start training from score 1.142055\n",
      "[LightGBM] [Info] Start training from score 1.140744\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier  # LightGBM 분류 모델 임포트\n",
    "\n",
    "# LightGBM 모델 생성: random_state=42로 설정해 결과의 재현성을 보장\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "\n",
    "# 교차 검증 수행\n",
    "# 학습 세트(train_input, train_target)를 사용해 교차 검증을 수행하고, 각 훈련 및 검증 단계에서의 성능을 수집\n",
    "# return_train_score=True로 설정해 학습 단계에서의 점수도 반환\n",
    "# n_jobs=-1로 설정해 가능한 모든 CPU 코어를 사용해 병렬 처리\n",
    "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
